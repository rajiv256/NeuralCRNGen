{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff32dbcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: Package ProgressBars not found in current path.\n- Run `import Pkg; Pkg.add(\"ProgressBars\")` to install the ProgressBars package.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package ProgressBars not found in current path.\n- Run `import Pkg; Pkg.add(\"ProgressBars\")` to install the ProgressBars package.",
      "",
      "Stacktrace:",
      " [1] macro expansion",
      "   @ ./loading.jl:1163 [inlined]",
      " [2] macro expansion",
      "   @ ./lock.jl:223 [inlined]",
      " [3] require(into::Module, mod::Symbol)",
      "   @ Base ./loading.jl:1144",
      " [4] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "using Pkg;\n",
    "# Pkg.add(\"ReactionNetworkImporters\");\n",
    "# Pkg.add(\"Dictionaries\");\n",
    "# Pkg.add(\"LaTeXStrings\");\n",
    "# Pkg.add(\"Statistics\");\n",
    "# Pkg.add(\"ColorSchemes\");\n",
    "# Pkg.add(\"IterTools\"); \n",
    "# Pkg.add(\"NNlib\"); \n",
    "# Pkg.add(\"DifferentialEquations\");\n",
    "# # Pkg.add(\"Plots\");\n",
    "# Pkg.add(\"Formatting\");\n",
    "# Pkg.add(\"LinearAlgebra\");\n",
    "# Pkg.add(\"Noise\");\n",
    "# Pkg.add(\"Catalyst\");\n",
    "# Pkg.add(\"IJulia\");\n",
    "# Pkg.add(\"ProgressBars\");\n",
    "# Pkg.add(\"ProgressMeter\");\n",
    "\n",
    "using DifferentialEquations;\n",
    "using Random;\n",
    "using Formatting;\n",
    "using LinearAlgebra;\n",
    "using Noise;\n",
    "using ReactionNetworkImporters;\n",
    "using Dictionaries;\n",
    "using LaTeXStrings;\n",
    "using Statistics;\n",
    "using ColorSchemes;\n",
    "using Catalyst;\n",
    "using IterTools;\n",
    "using NNlib;\n",
    "using IJulia; \n",
    "using ProgressMeter;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93235185",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: ArgumentError: Package Distributions not found in current path.\n- Run `import Pkg; Pkg.add(\"Distributions\")` to install the Distributions package.\nin expression starting at /Users/rajiv/Desktop/PhD/neural-ode/NeuralCRNGen/julia/utils.jl:13\nin expression starting at /Users/rajiv/Desktop/PhD/neural-ode/NeuralCRNGen/julia/datasets.jl:13",
     "output_type": "error",
     "traceback": [
      "LoadError: ArgumentError: Package Distributions not found in current path.\n- Run `import Pkg; Pkg.add(\"Distributions\")` to install the Distributions package.\nin expression starting at /Users/rajiv/Desktop/PhD/neural-ode/NeuralCRNGen/julia/utils.jl:13\nin expression starting at /Users/rajiv/Desktop/PhD/neural-ode/NeuralCRNGen/julia/datasets.jl:13",
      "",
      "Stacktrace:",
      " [1] macro expansion",
      "   @ ./loading.jl:1163 [inlined]",
      " [2] macro expansion",
      "   @ ./lock.jl:223 [inlined]",
      " [3] require(into::Module, mod::Symbol)",
      "   @ Base ./loading.jl:1144",
      " [4] include(fname::String)",
      "   @ Base.MainInclude ./client.jl:476",
      " [5] top-level scope",
      "   @ ~/Desktop/PhD/neural-ode/NeuralCRNGen/julia/datasets.jl:13",
      " [6] include(fname::String)",
      "   @ Base.MainInclude ./client.jl:476",
      " [7] top-level scope",
      "   @ In[2]:1",
      " [8] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [9] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "include(\"datasets.jl\")\n",
    "include(\"utils.jl\")\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1dab94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequester_backward_variables (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function f(u, xAndp, t)\n",
    "    LU = length(u)\n",
    "    x = xAndp[1:LU]\n",
    "    p = xAndp[LU+1:end]\n",
    "    dims, theta, beta, w, h, _, _ = sequester_params(p)\n",
    "    hvec = [h, h, h]\n",
    "    fmat = hvec + (theta * x + beta) .* u - u .* u\n",
    "    # fmat = theta*u\n",
    "    @assert length(fmat) == length(u)\n",
    "    return fmat\n",
    "end\n",
    "\n",
    "function forward!(du, u, xAndp, t)\n",
    "    fmat = f(u, xAndp, t)\n",
    "    for i in eachindex(fmat)\n",
    "        du[i] = fmat[i]\n",
    "    end\n",
    "end\n",
    "\n",
    "# Calculates the final hidden state of the neural ode\n",
    "function forward_node(u0, xAndp, tspan)\n",
    "    prob = ODEProblem(forward!, u0, tspan, xAndp)\n",
    "    sol = solve(prob, Tsit5(), reltol=1e-8, abstol=1e-12, save_on=false)\n",
    "    return sol\n",
    "end\n",
    "\n",
    "\n",
    "# Final feedforward layer similar to a perceptron\n",
    "function forward_ffnet(z, w; threshold=nothing)\n",
    "    yhat = dot(w, z) # Verified!\n",
    "    # CHECK: Thinking of the final layer as a binary perceptron \n",
    "    # println(\"ODE | yhat at t=T: $yhat\")\n",
    "    return yhat\n",
    "end\n",
    "\n",
    "\n",
    "function forward_step(u0, p, w, tspan; threshold=nothing)\n",
    "    \n",
    "    xAndp = []\n",
    "    append!(xAndp, u0)\n",
    "    append!(xAndp, p)\n",
    "    \n",
    "    # Output from the neural ode\n",
    "    node_out = forward_node(u0, xAndp, tspan)\n",
    "    # Extracting hidden state\n",
    "    z = node_out.u[end][1:length(u0)]\n",
    "    \n",
    "    yhat = forward_ffnet(z, w, threshold=threshold)\n",
    "    return (z, yhat)\n",
    "end\n",
    "\n",
    "\n",
    "function sequester_backward_variables(s; dims=3)\n",
    "    offset = 0\n",
    "\n",
    "    z = s[1:dims]\n",
    "    z = reshape(z, (dims, 1))\n",
    "    offset += dims \n",
    "\n",
    "    adj = s[offset + 1: offset + dims]\n",
    "    offset += dims\n",
    "    adj = reshape(adj, (dims, 1))\n",
    "\n",
    "    gtheta = s[offset + 1: offset + dims^2]\n",
    "    offset + dims^2\n",
    "    gtheta = reshape(gtheta, (1, dims^2)) # TODO: Check this!\n",
    "\n",
    "    gbeta = s[offset + 1: offset + dims]\n",
    "    offset += dims\n",
    "    gbeta = reshape(gbeta, (dims, 1))\n",
    "\n",
    "    gdt = s[offset + 1]\n",
    "    return z, adj, gtheta, gbeta, gdt    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f393ad8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backpropagation_step (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function aug_dynamics!(du, u, sAndp, t)\n",
    "    \n",
    "    # Init all gradients and values with zeroes\n",
    "    for i in eachindex(du)\n",
    "        du[i] = 0.0\n",
    "    end\n",
    "\n",
    "    LU = length(u)\n",
    "    s0 = sAndp[1:LU]\n",
    "    p = sAndp[LU+1:end]\n",
    "    dims, theta, beta, w, h, _, _ = sequester_params(p)\n",
    "\n",
    "    zatT, aatT, _, _, _ = sequester_backward_variables(s0, dims=dims)\n",
    "    z, a, gtheta, gbeta, gdt = sequester_backward_variables(u, dims=dims)\n",
    "\n",
    "\n",
    "    ## Time dynamics for z\n",
    "    zAndp = []\n",
    "    append!(zAndp, zatT)\n",
    "    append!(zAndp, p)\n",
    "\n",
    "    dfdt = -f(z, zAndp, t)\n",
    "    @assert length(dfdt) == dims\n",
    "    offset = 0\n",
    "    for i in 1:dims\n",
    "        du[offset+i] = dfdt[i]\n",
    "    end\n",
    "    offset += dims\n",
    "\n",
    "    ## Time dynamics for the adjoint\n",
    "    a = reshape(a, (dims, 1))\n",
    "\n",
    "    # ‚àÇf/‚àÇz = I*xT*ùúÉT + betaT - 2Iz\n",
    "    dfdz = Diagonal(vec(-theta*zatT - beta + 2*z))\n",
    "    dadt = -transpose(a) * dfdz\n",
    "    dadt = reshape(dadt, (dims, 1))\n",
    "    for i in eachindex(dadt)\n",
    "        du[offset+i] = dadt[i]\n",
    "    end\n",
    "    offset += length(dadt)\n",
    "\n",
    "    ## Time dynamics for gradients\n",
    "    dfdtheta = zeros(dims, dims^2)\n",
    "    for i in 1:dims\n",
    "        for j in 1:dims\n",
    "            dfdtheta[i, (i-1)*dims+j] = -zatT[j]*z[i]\n",
    "        end\n",
    "    end\n",
    "    gtheta = -transpose(a) * dfdtheta\n",
    "\n",
    "    @assert size(gtheta) == (1, dims^2)\n",
    "    for i in eachindex(gtheta)\n",
    "        du[offset+i] = gtheta[i]\n",
    "    end\n",
    "    offset += length(gtheta)\n",
    "\n",
    "    ## Time dynamics for beta \n",
    "    dfdbeta = Diagonal(vec(-z))\n",
    "    gbeta = -transpose(a)*dfdbeta\n",
    "\n",
    "    for i in eachindex(gbeta)\n",
    "        du[offset + i] = gbeta[i]\n",
    "    end\n",
    "    # Time dynamics of time(!!): Not used though sigh.\n",
    "    # TODO: Might wanna change this in future if things don't work\n",
    "    \n",
    "    gdt = -transpose(a) * dfdt\n",
    "    @assert length(gdt) == 1\n",
    "    # currently not changing time!\n",
    "    du[offset+1] = 0\n",
    "end\n",
    "\n",
    "\n",
    "function backpropagation_step(s0, p, tspan; dims=3)\n",
    "    \n",
    "    sAndp = []\n",
    "    append!(sAndp, s0)\n",
    "    append!(sAndp, p)\n",
    "    prob = ODEProblem(aug_dynamics!, s0, tspan, sAndp)\n",
    "    sol = solve(prob, Tsit5(), reltol=1e-8, abstol=1e-12, save_on=false)\n",
    "    return sol\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c082ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node_main (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function training_step(x, y, p; threshold=nothing)\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: augmented input\n",
    "        y: output \n",
    "        p: parameters of the entire network\n",
    "    \"\"\"\n",
    "    \n",
    "    dims, theta, beta, w, h, t0, t1 = sequester_params(p)\n",
    "    tspan = (t0, t1)\n",
    "    \n",
    "    @assert length(w) == dims \n",
    "    \n",
    "    # Forward & Hidden state calculation\n",
    "    println(\"ODE | w | \", w)\n",
    "    z, yhat = forward_step(x, p, w, tspan, threshold=threshold)\n",
    "    z = reshape(z, (dims, 1)) # Make z a row-vector\n",
    "    println(\"ODE | z at t=T | \", z)\n",
    "    println(\"ODE | expected yhat | \", sum(z .* w))\n",
    "    println(\"ODE | yhat at t=T | \", yhat)\n",
    "    println(\"ODE | y | \", y)\n",
    "    \n",
    "    # Loss\n",
    "    loss = 0.5*(yhat-y)^2\n",
    "\n",
    "    println(\"ODE | loss | \", loss)\n",
    "    \n",
    "\n",
    "    ####### backpropagation_step #############\n",
    "    zAndp = []\n",
    "    append!(zAndp, z)\n",
    "    append!(zAndp, p)\n",
    "\n",
    "    # Adjoint calculation\n",
    "    a = (yhat-y)*w\n",
    "    a = reshape(a, (dims, 1))\n",
    "    println(\"ODE | yhat at t=T | \", yhat)\n",
    "    println(\"ODE | Adjoint at t=T | \", a)\n",
    "    \n",
    "    # Initial theta gradients\n",
    "    gtheta = zeros(dims^2, 1)\n",
    "\n",
    "    # Initial beta gradients\n",
    "    gbeta = zeros(dims)\n",
    "    \n",
    "    # Initial time gradients \n",
    "    bfunc = -f(z, zAndp, tspan)\n",
    "\n",
    "    # These will be set to zero nevertheless\n",
    "    dldt1 = -transpose(a)*bfunc\n",
    "    gdt = convert(Array{Float64}, dldt1)\n",
    "    \n",
    "    ## Gradients wrt w\n",
    "    wgrads = (yhat - y) * z\n",
    "    println(\"ODE | error: \", yhat - y)\n",
    "\n",
    "    # Initial state for the reverse time ODE\n",
    "    s0 = vcat(z, a, gtheta, gbeta, dldt1)\n",
    "    backward = backpropagation_step(s0, p, tspan)\n",
    "    zat0, aat0, gtheta, gbeta, gdt = sequester_backward_variables(backward[end], dims=dims)\n",
    "    \n",
    "    # println(\"ODE | G: Gradients at t=0 | \", gradients)\n",
    "    gradients = (zat0, aat0, gtheta, gbeta, wgrads, dldt1)\n",
    "    return z, yhat, loss, gradients\n",
    "end\n",
    "\n",
    "function node_main(params, train, val; DIMS=3, EPOCHS=20, LR=0.001, threshold=nothing)\n",
    "    # Begin the training process\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    p = Progress(EPOCHS; dt=1.0)\n",
    "\n",
    "    for epoch in 1:EPOCHS\n",
    "        epoch_loss = 0.0\n",
    "        for i in eachindex(train)\n",
    "            println(\"=========EPOCH: $epoch | ITERATION: $i ===========\")\n",
    "            x, y = get_one(train, i)\n",
    "            \n",
    "            # Augment\n",
    "            x = augment(x, DIMS-length(x))\n",
    "            \n",
    "            # Run a training step\n",
    "            println(\"ODE | Input: $x | Target: $y\")\n",
    "            _, theta, beta, w, h, t0, t1 = sequester_params(params)\n",
    "            println(\"ODE | params before | \", params)\n",
    "            println(\"ODE | Ideal ReLU | \", relu.(x + theta * x + beta))\n",
    "            z, yhat, loss, gradients = training_step(x, y, params, threshold=threshold)\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            z0, a0, gtheta, gbeta, wgrads, gdt = gradients\n",
    "            \n",
    "            println(\"ODE | z at t=0 | \", z0)\n",
    "            println(\"ODE | a at t=0 | \", a0)\n",
    "            println(\"ODE | gtheta | \", gtheta)\n",
    "            println(\"ODE | gbeta | \", gbeta)\n",
    "            println(\"ODE | wgrads | \", wgrads)\n",
    "            println(\"ODE | gdt | \", gdt)\n",
    "\n",
    "            # Update params\n",
    "            offset = 1\n",
    "            for i in eachindex(gtheta)\n",
    "                params[offset+i] = params[offset + i] - LR*gtheta[i]\n",
    "            end\n",
    "            offset += length(gtheta)\n",
    "            for i in eachindex(gbeta)\n",
    "                params[offset + i] = params[offset + i] - LR*gbeta[i]\n",
    "            end\n",
    "            offset += length(gbeta)\n",
    "            for i in eachindex(wgrads)\n",
    "                params[offset + i] = params[offset + i] - LR*wgrads[i]\n",
    "            end\n",
    "            offset += length(wgrads)\n",
    "            offset += 1 # for h\n",
    "            offset += 1 # for t0\n",
    "            ## TODO: Not changing time for now!\n",
    "            # for i in eachindex(gdt)\n",
    "            #     params[offset + i] = gdt[i]\n",
    "            # end\n",
    "            println(\"ODE | params after | \", params)\n",
    "        end\n",
    "        \n",
    "        epoch_loss /= length(train)\n",
    "        push!(losses, epoch_loss)\n",
    "        lossplts = plot(losses)\n",
    "        png(lossplts, \"images/trainlossplts.png\")\n",
    "        accuracy = 0.0\n",
    "        val_epoch_loss = 0.0\n",
    "        before = []\n",
    "        after = []\n",
    "        yhats = []\n",
    "        for v in eachindex(val)\n",
    "            println(\"=======VAL Epoch: $epoch | ITERATION: $v\")\n",
    "            x, y = get_one(val, v)\n",
    "\n",
    "            # Augment\n",
    "            x = augment(x, DIMS - length(x))\n",
    "\n",
    "            _, theta, beta, w, h, t0, t1= sequester_params(params)\n",
    "            tspan = (t0, t1)\n",
    "            @assert length(w) == DIMS\n",
    "\n",
    "            before_tmp = []\n",
    "            append!(before_tmp, x)\n",
    "            push!(before_tmp, y)\n",
    "            push!(before, before_tmp)\n",
    "\n",
    "            println(\"ODE | Input: $x | Target: $y\")\n",
    "            println(\"params before | \", params)\n",
    "            z, yhat = forward_step(x, params, w, tspan, threshold=threshold)\n",
    "\n",
    "            loss = 0.5 * (yhat - y)^2\n",
    "        \n",
    "            class = yhat/abs(yhat)\n",
    "            \n",
    "            after_tmp = []\n",
    "            append!(after_tmp, z)\n",
    "            push!(after_tmp, y)\n",
    "            push!(after, after_tmp)\n",
    "\n",
    "\n",
    "            val_epoch_loss += loss\n",
    "\n",
    "            yhats_tmp = []\n",
    "            append!(yhats_tmp, x)\n",
    "            push!(yhats_tmp, class)\n",
    "            push!(yhats, yhats_tmp)\n",
    "\n",
    "            if class == y\n",
    "                accuracy += 1\n",
    "            end\n",
    "        end\n",
    "        accuracy  = accuracy / length(val)\n",
    "        println(\"accuracy: \", accuracy)\n",
    "        push!(val_accuracies, accuracy)\n",
    "        valaccplot = plot(val_accuracies)\n",
    "        \n",
    "        if DIMS == 2\n",
    "            beforeplt = scatter(getindex.(before, 1), getindex.(before, 2), group=getindex.(before, 3))\n",
    "            afterplot = scatter(getindex.(after, 1), getindex.(after, 2), group=getindex.(after, 3))\n",
    "            yhatplt = scatter(getindex.(yhats, 1), getindex.(yhats, 2), group=getindex.(yhats, 3))\n",
    "        end\n",
    "        if DIMS==3\n",
    "            beforeplt = scatter3d(getindex.(before, 1), getindex.(before, 2), getindex.(before, 3), group=getindex.(before, 4))\n",
    "            afterplot = scatter3d(getindex.(after, 1), getindex.(after, 2), getindex.(after, 3), group=getindex.(after, 4))\n",
    "            yhatplt = scatter3d(getindex.(yhats, 1), getindex.(yhats, 2), getindex.(yhats, 3), group=getindex.(yhats, 4))\n",
    "        end\n",
    "        png(beforeplt, \"images/ode_before.png\")\n",
    "        png(afterplot, \"images/ode_after.png\")\n",
    "        png(valaccplot, \"images/ode_val_accuracies.png\")\n",
    "        png(yhatplt, \"images/ode_yhats.png\")\n",
    "        next!(p)\n",
    "\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15121ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: create_annular_rings_dataset not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: create_annular_rings_dataset not defined",
      "",
      "Stacktrace:",
      " [1] neuralode(; DIMS::Int64)",
      "   @ Main ./In[6]:4",
      " [2] neuralode()",
      "   @ Main ./In[6]:1",
      " [3] top-level scope",
      "   @ In[6]:16",
      " [4] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    " function neuralode(; DIMS=3)\n",
    "    # train = create_linearly_separable_dataset(100, linear, threshold=0.0)\n",
    "    # val = create_linearly_separable_dataset(40, linear, threshold=0.0)\n",
    "    train = create_annular_rings_dataset(200)\n",
    "    val = create_annular_rings_dataset(50)  \n",
    "    # val = train   \n",
    "\n",
    "    params_orig = create_node_params(DIMS, t0=0.0, t1=0.6, h=0.5)\n",
    "    # for i in 1:length(params_orig)\n",
    "    #     params_orig[i] = abs(params_orig[i])\n",
    "    # end\n",
    "    \n",
    "    node_main(params_orig, train, val, DIMS=DIMS, EPOCHS=100, threshold=0.0, LR=0.01)\n",
    "end\n",
    "\n",
    "neuralode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307351f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f0cd7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28122335464066994"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.28191304882618096 - 0.001 * 0.6896941855110088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fbc159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10e6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
